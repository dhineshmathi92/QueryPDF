{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dedb721-361f-4a4b-afbb-2373dccdca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"./.env\")\n",
    "# OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "# GOOGLE_API_KEY = os.environ['GOOGLE_API_KEY']\n",
    "HF_TOKEN = os.environ['HUGGING_FACE_TOKEN']\n",
    "\n",
    "# Load the documents from local\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_dir=\"C:/Users/dhinesh.m/Documents/Practical_RAG/data\" , required_exts=['.pdf']).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7bea604-0745-41a4-b123-61489cabe199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of documents 50\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of documents\", len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30779c6f-8d82-421e-b3d7-48bb7d90bf04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34218eff65cc47c0b68cfd5397fb7d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Chunking the documents for embedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=512 , chunk_overlap=20)\n",
    "nodes = splitter.get_nodes_from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08d5bcc9-d7e1-41e8-8faf-01e261672d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfab8ac-f9f7-4eed-950a-8991a1f8af3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install llama-index-embeddings-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "884351db-a041-4db5-be51-4fd17ccd55e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "# Creating a faiss vector store with openai embedding\n",
    "# from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "# from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.settings import Settings\n",
    "import faiss\n",
    "\n",
    "# embed_model = OpenAIEmbedding(model = 'text-embedding-ada-002', api_key = OPENAI_API_KEY)\n",
    "# embed_model = GoogleGenAIEmbedding(model_name = 'text-embedding-004', api_key = GOOGLE_API_KEY)\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"nomic-ai/nomic-embed-text-v1\",trust_remote_code=True)\n",
    "Settings.embed_model = embed_model\n",
    "embed_dimension= 768\n",
    "faiss_index = faiss.IndexFlatL2(embed_dimension)\n",
    "\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "llm = HuggingFaceInferenceAPI(model=\"mistralai/Mistral-7B-Instruct-v0.3\", token=HF_TOKEN)\n",
    "Settings.llm = llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acb24a6e-b976-41fe-b5f4-afa67370066c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb823d2a45e4f4dba6041a9de5b7a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating vector store and indexing the document chunks(nodes)\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "\n",
    "vector_store = FaissVectorStore(faiss_index = faiss_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store = vector_store)\n",
    "index = VectorStoreIndex(nodes = nodes, embed_model = embed_model, storage_context = storage_context, show_progress = True )\n",
    "\n",
    "# Persist the index and vector store to disk\n",
    "index.storage_context.persist(persist_dir=\"./storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185e343d-796b-446b-88e0-54e0bcb50b20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "\n",
    "# # llm = Gemini(model=\"models/gemini-1.5-flash\",api_key= GOOGLE_API_KEY)\n",
    "# llm = HuggingFaceInferenceAPI(model=\"mistralai/Mistral-7B-Instruct-v0.3\", token=HF_TOKEN)\n",
    "\n",
    "# query_engine = index.as_chat_engine(llm = llm, similarity_top_k=3)\n",
    "# result = query_engine.query(\"what is transformer attention?\")\n",
    "# print(result.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f14e25b-1a83-4704-badc-92c136b71f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the index from persisted directory\n",
    "from llama_index.core import load_index_from_storage\n",
    "\n",
    "vector_store = FaissVectorStore.from_persist_dir(\"./storage\")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store, persist_dir=\"./storage\"\n",
    ")\n",
    "index = load_index_from_storage(storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639493d3-9ccb-4d39-bdab-92561a16d8e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install llama-index-llms-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "155e1dca-b861-443d-8cfe-a2e89b3b4f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# llm = OpenAI(model = 'gpt-3.5-turbo', api_key = OPENAI_API_KEY)\n",
    "\n",
    "# from llama_index.llms.gemini import Gemini\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "\n",
    "# llm = Gemini(model=\"models/gemini-1.5-flash\",api_key= GOOGLE_API_KEY)\n",
    "llm = HuggingFaceInferenceAPI(model=\"mistralai/Mistral-7B-Instruct-v0.3\", token=HF_TOKEN)\n",
    "\n",
    "# Creating a prompt template\n",
    "\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "# Define the prompt template\n",
    "qa_prompt = PromptTemplate(\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, answer the question: {query_str}\"\n",
    ")\n",
    "\n",
    "\n",
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "# Initialize the response synthesizer with the custom prompt\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    llm = llm,\n",
    "    text_qa_template = qa_prompt,\n",
    "    response_mode=\"compact\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e25fed1-f7d3-4af7-8587-482488061d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever with similarity_top_k=5\n",
    "retriever = index.as_retriever(similarity_top_k=3)\n",
    "\n",
    "\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# Initialize the query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4195d608-aeb8-40e5-8ffe-60e01567dfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  The main components of a RAG model are a retriever and a generator. The retriever, denoted as p η(z |x ), is a component that uses the input sequence x to retrieve text documents z and returns distributions over text passages given a query x. The generator, denoted as p θ(y i |x,z,y 1:i −1), is a parametrized component that generates the target sequence y based on the input sequence x, the retrieved text documents z, and the previously generated sequence y 1:i −1.\n",
      "\n",
      "The retriever component in a RAG model is non-parametric, meaning it does not have learnable parameters. Instead, it relies on a retrieval strategy to select relevant text documents from a large corpus. The generator component, on the other hand, is parametric and learns to generate sequences based on the input and context provided by the retriever.\n",
      "\n",
      "In terms of how they react, the retriever component retrieves text documents based on the input query, while the generator component generates sequences based on the input, the retrieved context, and the previously generated sequence. The retriever component can be updated by\n",
      "\n",
      "Source details: \n",
      "{'page_label': '2', 'file_name': '2005.11401v4.pdf', 'file_path': 'C:\\\\Users\\\\dhinesh.m\\\\Documents\\\\Practical_RAG\\\\data\\\\2005.11401v4.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-05-23', 'last_modified_date': '2025-05-23'}\n",
      "Sample text:  For FEVER [56] fact veriﬁcation, we achieve results within 4.3% of\n",
      "state-of-the-art pipeline models \n",
      "\n",
      "{'page_label': '1', 'file_name': '2005.11401v4.pdf', 'file_path': 'C:\\\\Users\\\\dhinesh.m\\\\Documents\\\\Practical_RAG\\\\data\\\\2005.11401v4.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-05-23', 'last_modified_date': '2025-05-23'}\n",
      "Sample text:  For language generation tasks, we ﬁnd that RAG models generate\n",
      "more speciﬁc, diverse and factual lan\n",
      "\n",
      "{'page_label': '8', 'file_name': '2005.11401v4.pdf', 'file_path': 'C:\\\\Users\\\\dhinesh.m\\\\Documents\\\\Practical_RAG\\\\data\\\\2005.11401v4.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-05-23', 'last_modified_date': '2025-05-23'}\n",
      "Sample text:  As FEVER is a classiﬁcation task, both RAG models are equivalent.\n",
      "Model NQ TQA WQ CT Jeopardy-QGen M\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = query_engine.query(\"What are main components of a RAG model and how do they react ?\")\n",
    "\n",
    "print(\"Response: \", result.response, end=\"\\n\\n\")\n",
    "print(\"Source details: \", end=\"\\n\")\n",
    "for src in result.source_nodes:\n",
    "    print(src.node.metadata)\n",
    "    print(\"Sample text: \", src.node.text[:100])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f755b737-f616-40e7-8178-115e32a1b642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  The two sub-layers in each encoder block of the Transformer model are a multi-head self-attention mechanism and a simple, position-wise fully connected feed-forward network.\n",
      "\n",
      "Source details: \n",
      "{'page_label': '3', 'file_name': 'transformers.pdf', 'file_path': 'C:\\\\Users\\\\dhinesh.m\\\\Documents\\\\Practical_RAG\\\\data\\\\transformers.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-05-06', 'last_modified_date': '2025-05-06'}\n",
      "Sample text:  Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture us\n",
      "\n",
      "{'page_label': '5', 'file_name': 'transformers.pdf', 'file_path': 'C:\\\\Users\\\\dhinesh.m\\\\Documents\\\\Practical_RAG\\\\data\\\\transformers.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-05-06', 'last_modified_date': '2025-05-06'}\n",
      "Sample text:  See Figure 2.\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "In addition to attention sub-layers, each of t\n",
      "\n",
      "{'page_label': '5', 'file_name': 'transformers.pdf', 'file_path': 'C:\\\\Users\\\\dhinesh.m\\\\Documents\\\\Practical_RAG\\\\data\\\\transformers.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-05-06', 'last_modified_date': '2025-05-06'}\n",
      "Sample text:  output values. These are concatenated and once again projected, resulting in the final values, as\n",
      "de\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = query_engine.query(\"What are two sub layers in each encoder block of the transformer model ?\")\n",
    "\n",
    "print(\"Response: \", result.response, end=\"\\n\\n\")\n",
    "print(\"Source details: \", end=\"\\n\")\n",
    "for src in result.source_nodes:\n",
    "    print(src.node.metadata)\n",
    "    print(\"Sample text: \", src.node.text[:100])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c725ca71-069d-47fd-86f6-ce50fe0fdfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  In the Transformer model, positional encoding is implemented to provide the model with information about the relative or absolute position of the tokens in the sequence, as the model itself does not have any recurrence or convolution to inherently understand the order of the sequence.\n",
      "\n",
      "The positional encodings have the same dimension as the embeddings, allowing them to be summed with the input embeddings at the bottoms of the encoder and decoder stacks. In this work, the authors use sine and cosine functions of different frequencies for the positional encodings. Each dimension of the positional encoding corresponds to a sinusoid, and the wavelengths form a geometric progression from 2π to 10000 · 2π. This function is chosen because it allows the model to easily learn to attend by relative positions, as for any fixed offset k, the positional encoding at position pos+k can be represented as a linear function of the positional encoding at position pos.\n",
      "\n",
      "Positional encoding is necessary because the Transformer model is a purely attention-based model that does not have any inherent sense of position or order. By adding positional encodings, the model can make use of the\n",
      "\n",
      "Source details: \n",
      "{'page_label': '3', 'file_name': 'transformers.pdf', 'file_path': 'C:\\\\Users\\\\dhinesh.m\\\\Documents\\\\Practical_RAG\\\\data\\\\transformers.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-05-06', 'last_modified_date': '2025-05-06'}\n",
      "Sample text:  Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture us\n",
      "\n",
      "{'page_label': '6', 'file_name': 'transformers.pdf', 'file_path': 'C:\\\\Users\\\\dhinesh.m\\\\Documents\\\\Practical_RAG\\\\data\\\\transformers.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-05-06', 'last_modified_date': '2025-05-06'}\n",
      "Sample text:  Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
      "for \n",
      "\n",
      "{'page_label': '5', 'file_name': 'transformers.pdf', 'file_path': 'C:\\\\Users\\\\dhinesh.m\\\\Documents\\\\Practical_RAG\\\\data\\\\transformers.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-05-06', 'last_modified_date': '2025-05-06'}\n",
      "Sample text:  See Figure 2.\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "In addition to attention sub-layers, each of t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = query_engine.query(\"Explain how positional encoding is implemented in transformer and why is it necessary?\")\n",
    "\n",
    "print(\"Response: \", result.response, end=\"\\n\\n\")\n",
    "print(\"Source details: \", end=\"\\n\")\n",
    "for src in result.source_nodes:\n",
    "    print(src.node.metadata)\n",
    "    print(\"Sample text: \", src.node.text[:100])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c786ed1b-c767-4d85-9c24-647474fb6751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  Multi-head attention in Transformer is a mechanism that allows the model to jointly attend to information from different representation subspaces at different positions. This is achieved by dividing the attention process into multiple parallel heads, each with a reduced dimension compared to the full dimension.\n",
      "\n",
      "In the Transformer, there are h = 8 parallel attention layers, or heads. For each head, the queries (Q), keys (K), and values (V) are projected through separate parameter matrices (WQi, WKi, WVi) before being used in the attention calculation. The outputs of all heads are then concatenated and linearly transformed by another parameter matrix (WO) to produce the final output.\n",
      "\n",
      "The benefit of using multi-head attention is that it allows the model to capture information from different perspectives simultaneously, reducing the risk of averaging inhibiting this as with a single attention head. By using multiple heads, the model can learn to attend to different aspects of the input sequence, improving its ability to understand and generate complex sequences.\n",
      "\n",
      "Additionally, the reduced dimension of each head results in a similar computational cost as single-head attention with full dimensionality, making the model more efficient. This is because the total comput\n",
      "\n",
      "Source details: \n",
      "{'page_label': '5', 'file_name': 'transformers.pdf', 'file_path': 'C:\\\\Users\\\\dhinesh.m\\\\Documents\\\\Practical_RAG\\\\data\\\\transformers.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-05-06', 'last_modified_date': '2025-05-06'}\n",
      "Sample text:  output values. These are concatenated and once again projected, resulting in the final values, as\n",
      "de\n",
      "\n",
      "{'page_label': '2', 'file_name': 'transformers.pdf', 'file_path': 'C:\\\\Users\\\\dhinesh.m\\\\Documents\\\\Practical_RAG\\\\data\\\\transformers.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-05-06', 'last_modified_date': '2025-05-06'}\n",
      "Sample text:  This makes\n",
      "it more difficult to learn dependencies between distant positions [ 12]. In the Transform\n",
      "\n",
      "{'page_label': '1', 'file_name': 'transformers.pdf', 'file_path': 'C:\\\\Users\\\\dhinesh.m\\\\Documents\\\\Practical_RAG\\\\data\\\\transformers.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-05-06', 'last_modified_date': '2025-05-06'}\n",
      "Sample text:  Ashish, with Illia, designed and implemented the first Transformer models and\n",
      "has been crucially inv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = query_engine.query(\"Describe the concept of multi-head attention in transformer and why is it beneficial ?\")\n",
    "\n",
    "print(\"Response: \", result.response, end=\"\\n\\n\")\n",
    "print(\"Source details: \", end=\"\\n\")\n",
    "for src in result.source_nodes:\n",
    "    print(src.node.metadata)\n",
    "    print(\"Sample text: \", src.node.text[:100])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e2be7aa-093e-4e0a-9501-36f976537757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  Few-shot learning is a concept in machine learning where a model is trained on a small number of examples (few shots) and is then expected to generalize and perform well on new, unseen examples that are similar to the training data.\n",
      "\n",
      "Regarding GPT-3, the context information does not provide specific details about its implementation of few-shot learning during inference. However, it is known that GPT-3 is a transformer-based model that uses a large-scale pretraining approach on a diverse range of text data. During inference, it can generate responses based on the input it receives, and it can leverage the knowledge it has learned during pretraining to perform well on new, unseen tasks with only a few examples (few-shot learning). This is due to the model's ability to capture patterns and relationships in the data, allowing it to generalize to new tasks.\n",
      "\n",
      "Source details: \n",
      "{'page_label': '7', 'file_name': 'transformers.pdf', 'file_path': 'C:\\\\Users\\\\dhinesh.m\\\\Documents\\\\Practical_RAG\\\\data\\\\transformers.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-05-06', 'last_modified_date': '2025-05-06'}\n",
      "Sample text:  Sentence pairs were batched together by approximate sequence length. Each training\n",
      "batch contained a\n",
      "\n",
      "{'page_label': '19', 'file_name': '2005.11401v4.pdf', 'file_path': 'C:\\\\Users\\\\dhinesh.m\\\\Documents\\\\Practical_RAG\\\\data\\\\2005.11401v4.pdf', 'file_type': 'application/pdf', 'file_size': 885323, 'creation_date': '2025-05-23', 'last_modified_date': '2025-05-23'}\n",
      "Sample text:  Table 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluat\n",
      "\n",
      "{'page_label': '3', 'file_name': 'transformers.pdf', 'file_path': 'C:\\\\Users\\\\dhinesh.m\\\\Documents\\\\Practical_RAG\\\\data\\\\transformers.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2025-05-06', 'last_modified_date': '2025-05-06'}\n",
      "Sample text:  Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = query_engine.query(\"What is few-shot learning and how does GPT 3 implement it during inference ?\")\n",
    "\n",
    "print(\"Response: \", result.response, end=\"\\n\\n\")\n",
    "print(\"Source details: \", end=\"\\n\")\n",
    "for src in result.source_nodes:\n",
    "    print(src.node.metadata)\n",
    "    print(\"Sample text: \", src.node.text[:100])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e905bb44-94d0-42ec-aead-1de1d4ff7244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
